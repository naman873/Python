{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIMIZERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is Gradient Descent?\n",
    "\n",
    "To explain Gradient Descent I’ll use the classic mountaineering example.\n",
    "\n",
    "Suppose you are at the top of a mountain, and you have to reach a lake which is at the lowest point of the mountain (a.k.a valley). A twist is that you are blindfolded and you have zero visibility to see where you are headed. So, what approach will you take to reach the lake?\n",
    "\n",
    "\n",
    "\n",
    "The best way is to check the ground near you and observe where the land tends to descend. This will give an idea in what direction you should take your first step. If you follow the descending path, it is very likely you would reach the lake.\n",
    "\n",
    "To represent this graphically, notice the below graph.\n",
    "\n",
    "\n",
    "\n",
    "Let us now map this scenario in mathematical terms.\n",
    "\n",
    "Suppose we want to find out the best parameters (θ1) and (θ2) for our learning algorithm. Similar to the analogy above, we see we find similar mountains and valleys when we plot our “cost space”. Cost space is nothing but how our algorithm would perform when we choose a particular value for a parameter.\n",
    "\n",
    "So on the y-axis, we have the cost J(θ) against our parameters θ1 and θ2 on x-axis and z-axis respectively. Here, hills are represented by red region, which have high cost, and valleys are represented by blue region, which have low cost.\n",
    "\n",
    "Now there are many types of gradient descent algorithms. They can be classified by two methods mainly:\n",
    "\n",
    "On the basis of data ingestion\n",
    "Full Batch Gradient Descent Algorithm\n",
    "Stochastic Gradient Descent Algorithm\n",
    "In full batch gradient descent algorithms, you use whole data at once to compute the gradient, whereas in stochastic you take a sample while computing the gradient.\n",
    "\n",
    "On the basis of differentiation techniques \n",
    "First order Differentiation\n",
    "Second order Differentiation\n",
    "Gradient descent requires calculation of gradient by differentiation of cost function. We can either use first order differentiation or second order differentiation.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    " \n",
    "2. Challenges in executing Gradient Descent\n",
    "Gradient Descent is a sound technique which works in most of the cases. But there are many cases where gradient descent does not work properly or fails to work altogether. There are three main reasons when this would happen:\n",
    "\n",
    "Data challenges\n",
    "Gradient challenges\n",
    "Implementation challenges\n",
    " \n",
    "\n",
    "2.1 Data Challenges\n",
    "If the data is arranged in a way that it poses a non-convex optimization problem. It is very difficult to perform optimization using gradient descent. Gradient descent only works for problems which have a well defined convex optimization problem.\n",
    "Even when optimizing a convex optimization problem, there may be numerous minimal points. The lowest point is called global minimum, whereas rest of the points are called local minima. Our aim is to go to global minimum while avoiding local minima.\n",
    "There is also a saddle point problem. This is a point in the data where the gradient is zero but is not an optimal point. We don’t have a specific way to avoid this point and is still an active area of research.\n",
    " \n",
    "\n",
    "2.2 Gradient Challenges\n",
    "If the execution is not done properly while using gradient descent, it may lead to problems like vanishing gradient or exploding gradient problems. These problems occur when the gradient is too small or too large. And because of this problem the algorithms do not converge.\n",
    " \n",
    "\n",
    "2.3 Implementation Challenges\n",
    "Most of the neural network practitioners don’t generally pay attention to implementation, but it’s very important to look at the resource utilization by networks. For eg: When implementing gradient descent, it is very important to note how many resources you would require. If the memory is too small for your application, then the network would fail.\n",
    "Also, its important to keep track of things like floating point considerations and hardware / software prerequisites.\n",
    " \n",
    "\n",
    "3. Variants of Gradient Descent algorithms\n",
    "\n",
    "\n",
    "Let us look at most commonly used gradient descent algorithms and their implementations.\n",
    "\n",
    "3.1 Vanilla Gradient Descent\n",
    "\n",
    "\n",
    "This is the simplest form of gradient descent technique. Here, vanilla means pure / without any adulteration. Its main feature is that we take small steps in the direction of the minima by taking gradient of the cost function.\n",
    "\n",
    "Let’s look at its pseudocode.\n",
    "\n",
    "\n",
    "update = learning_rate * gradient_of_parameters\n",
    "\n",
    "parameters = parameters - update\n",
    "\n",
    "\n",
    "Here, we see that we make an update to the parameters by taking gradient of the parameters. And multiplying it by a learning rate, which is essentially a constant number suggesting how fast we want to go the minimum. Learning rate is a hyper-parameter and should be treated with care when choosing its value.\n",
    "\n",
    " \n",
    "\n",
    "3.2 Gradient Descent with Momentum\n",
    "Here, we tweak the above algorithm in such a way that we pay heed to the prior step before taking the next step.\n",
    "\n",
    "Here’s a pseudocode.\n",
    "\n",
    "update = learning_rate * gradient\n",
    "\n",
    "velocity = previous_update * momentum\n",
    "\n",
    "parameter = parameter + velocity – update\n",
    "\n",
    "\n",
    "\n",
    "Here, our update is the same as that of vanilla gradient descent. But we introduce a new term called velocity, which considers the previous update and a constant which is called momentum.\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "3.3 ADAGRAD\n",
    "ADAGRAD uses adaptive technique for learning rate updation. In this algorithm, on the basis of how the gradient has been changing for all the previous iterations we try to change the learning rate.\n",
    "\n",
    "Here’s a pseudocode\n",
    "\n",
    "grad_component = previous_grad_component + (gradient * gradient)\n",
    "\n",
    "rate_change = square_root(grad_component) + epsilon\n",
    "\n",
    "adapted_learning_rate = learning_rate * rate_change\n",
    "\n",
    "update = adapted_learning_rate * gradient\n",
    "\n",
    "parameter = parameter – update\n",
    "\n",
    "\n",
    "\n",
    "In the above code, epsilon is a constant which is used to keep rate of change of learning rate in check.\n",
    "\n",
    " \n",
    "\n",
    "3.4 ADAM\n",
    "ADAM is one more adaptive technique which builds on adagrad and further reduces it downside. In other words, you can consider this as momentum + ADAGRAD.\n",
    "\n",
    "Here’s a pseudocode.\n",
    "\n",
    "adapted_gradient = previous_gradient + ((gradient – previous_gradient) * (1 – beta1))\n",
    "\n",
    "gradient_component = (gradient_change – previous_learning_rate)\n",
    "adapted_learning_rate =  previous_learning_rate + (gradient_component * (1 – beta2))\n",
    "update = adapted_learning_rate * adapted_gradient\n",
    "parameter = parameter – update\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Here beta1 and beta2 are constants to keep changes in gradient and learning rate in check\n",
    "\n",
    "There are also second order differentiation method like l-BFGS. You can see an implementation of this algorithm in scipy library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence summing up, a derivative is simply defined for a function dependent on single variables , whereas a Gradient is defined for function dependent on multiple variables. Now let’s not get more into Calculas and Physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent\n",
    "Gradient Descent is the most important technique and the foundation of how we train and optimize Intelligent Systems. What is does is —\n",
    "“Oh Gradient Descent — Find the Minima , control the variance and then update the Model’s parameters and finally lead us to Convergence”\n",
    "\n",
    "\n",
    "θ=θ−η⋅∇J(θ) — is the formula of the parameter updates, where ‘η’ is the learning rate ,’∇J(θ)’ is the Gradient of Loss function-J(θ) w.r.t parameters-‘θ’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent(SGD)\n",
    "\n",
    "\n",
    "on the other hand performs a parameter update for each training example .It is usually much faster technique.It performs one update at a time.\n",
    "θ=θ−η⋅∇J(θ;x(i);y(i)) , where {x(i) ,y(i)} are the training examples.\n",
    "\n",
    "\n",
    "  Now due to these frequent updates ,parameters updates have high variance and causes the Loss function to fluctuate to different intensities. This is actually a good thing because it helps us discover new and possibly better local minima , whereas Standard Gradient Descent will only converge to the minimum of the basin as mentioned above.\n",
    "\n",
    "\n",
    "But the problem with SGD is that due to the frequent updates and fluctuations it ultimately complicates the convergence to the exact minimum and will keep overshooting due to the frequent fluctuations .\n",
    "Although, it has been shown that as we slowly decrease the learning rate-η, SGD shows the same convergence pattern as Standard gradient descent.\n",
    "\n",
    "High Variance parameter updates for each training example cause the Loss function to fluctuate heavily due to which we might not get the minimum value of parameter which gives us least Loss value.\n",
    "The problems of high variance parameter updates and unstable convergence can be rectified in another variant called Mini-Batch Gradient Descent.\n",
    "\n",
    "\n",
    "2. Mini Batch Gradient Descent\n",
    "\n",
    "\n",
    "An improvement to avoid all the problems and demerits of SGD and standard Gradient Descent would be to use Mini Batch Gradient Descent as it takes the best of both techniques and performs an update for every batch with n training examples in each batch.\n",
    "The advantages of using Mini Batch Gradient Descent are —\n",
    "It Reduces the variance in the parameter updates , which can ultimately lead us to a much better and stable convergence.\n",
    "Can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient.\n",
    "Commonly Mini-batch sizes Range from 50 to 256, but can vary as per the application and problem being solved.\n",
    "Mini-batch gradient descent is typically the algorithm of choice when training a neural network nowadays\n",
    "P.S —Actually the term SGD is used also when mini-batch gradient descent is used ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum\n",
    "\n",
    "The high variance oscillations in SGD makes it hard to reach convergence , so a technique called Momentum was invented which accelerates SGD by navigating along the relevant direction and softens the oscillations in irrelevant directions.In other words all it does is adds a fraction ‘γ’ of the update vector of the past step to the current update vector.\n",
    "\n",
    "V(t)=γV(t−1)+η∇J(θ).\n",
    "and finally we update parameters by θ=θ−V(t) .\n",
    "The momentum term γ is usually set to 0.9 or a similar value.\n",
    "Here the momentum is same as the momentum in classical physics , as we throw a ball down a hill it gathers momentum and its velocity keeps on increasing.\n",
    "The same thing happens with our parameter updates —\n",
    "It leads to faster and stable convergence.\n",
    "\n",
    "\n",
    "\n",
    "Reduced Oscillations\n",
    "The momentum term γ increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. This means it does parameter updates only for relevant examples. This reduces the unnecessary parameter updates which leads to faster and stable convergence and reduced oscillations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Adagrad\n",
    "\n",
    "It simply allows the learning Rate -η to adapt based on the parameters. So it makes big updates for infrequent parameters and small updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data.\n",
    "It uses a different learning Rate for every parameter θ at a time step based on the past gradients which were computed for that parameter.\n",
    "Previously, we performed an update for all parameters θ at once as every parameter θ(i) used the same learning rate η. As Adagrad uses a different learning rate for every parameter θ(i) at every time step t, we first show Adagrad’s per-parameter update, which we then vectorize. For brevity, we set g(t,i) to be the gradient of the loss function w.r.t. to the parameter θ(i) at time step t .\n",
    "\n",
    "The formula for Parameter updates\n",
    "Adagrad modifies the general learning rate η at each time step t for every parameter θ(i) based on the past gradients that have been computed for θ(i).\n",
    "The main benefit of Adagrad is that we don’t need to manually tune the learning Rate. Most implementations use a default value of 0.01 and leave it at that.\n",
    "Disadvantage —\n",
    "Its main weakness is that its learning rate-η is always Decreasing and decaying.\n",
    "\n",
    "\n",
    "This happens due to the accumulation of each squared Gradients in the denominator , since every added term is positive.The accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become so small, that the model just stops learning entirely and stops acquiring new additional knowledge. Because we know that as the learning rate gets smaller and smaller the ability of the Model to learn fastly decreases and which gives very slow convergence and takes very long to train and learn i.e learning speed suffers and decreases.\n",
    "\n",
    "This problem of Decaying learning Rate is Rectified in another algorithm called AdaDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam\n",
    "\n",
    "Adam stands for Adaptive Moment Estimation. Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients like AdaDelta ,Adam also keeps an exponentially decaying average of past gradients M(t), similar to momentum:\n",
    "\n",
    "M(t) and V(t) are values of the first moment which is the Mean and the second moment which is the uncentered variance of the gradients respectively.\n",
    "\n",
    "The formulas for the first Moment(mean) and the second moment (the variance) of the Gradients\n",
    "Then the final formula for the Parameter update is —\n",
    "\n",
    "The values for β1 is 0.9 , 0.999 for β2, and (10 x exp(-8)) for ϵ.\n",
    "Adam works well in practice and compares favorably to other adaptive learning-method algorithms as it converges very fast and the learning speed of the Model is quiet Fast and efficient and also it rectifies every problem that is faced in other optimization techniques such as vanishing Learning rate , slow convergence or High variance in the parameter updates which leads to fluctuating Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which optimizer should we use?\n",
    "\n",
    "The question was to choose the best optimizer for our Neural Network Model in order to converge fast and to learn properly and tune the internal parameters so as to minimize the Loss function .\n",
    "Adam works well in practice and outperforms other Adaptive techniques.\n",
    "\n",
    "If your input data is sparse then methods such as SGD,NAG and momentum are inferior and perform poorly. For sparse data sets one should use one of the adaptive learning-rate methods. An additional benefit is that we won’t need to adjust the learning rate but likely achieve the best results with the default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
